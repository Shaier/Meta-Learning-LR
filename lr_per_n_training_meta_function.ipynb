{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision.transforms.functional as TF\n",
    "import random \n",
    "import torch.nn.functional as F\n",
    "import math \n",
    "import functools\n",
    "\n",
    "plt.style.use('default')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "# seed = 888\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# helper functions\n",
    "\n",
    "mnist_trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((1.0,), (0.5,))])\n",
    "\n",
    "# if not exist, download mnist dataset\n",
    "root = './data'\n",
    "\n",
    "mnist_train_set = datasets.MNIST(root=root, train=True, transform=mnist_trans, download=True)\n",
    "mnist_test_set = datasets.MNIST(root=root, train=False, transform=mnist_trans, download=True)\n",
    "\n",
    "def rsetattr(obj, attr, val):\n",
    "    pre, _, post = attr.rpartition('.')\n",
    "    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n",
    "\n",
    "# using wonder's beautiful simplification: https://stackoverflow.com/questions/31174295/getattr-and-setattr-on-nested-objects/31174427?noredirect=1#comment86638618_31174427\n",
    "\n",
    "def rgetattr(obj, attr, *args):\n",
    "    def _getattr(obj, attr):\n",
    "        return getattr(obj, attr, *args)\n",
    "    return functools.reduce(_getattr, [obj] + attr.split('.'))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta model stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        # Initialize weights and biases to zero\n",
    "        # The line below is nearly identical to \"self.weight = ...\", but we get all of the added PyTorch features.\n",
    "        self.register_buffer('weight', torch.zeros(out_features, in_features, requires_grad=True))\n",
    "        if bias:\n",
    "            self.register_buffer('bias', torch.zeros(out_features, requires_grad=True))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "        # Fancy initialization from https://discuss.pytorch.org/t/how-are-layer-weights-and-biases-initialized-by-default/13073\n",
    "        stdv = 2. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_neurons = 64\n",
    "layer_2_neurons = 64\n",
    "layer_3_neurons = 64\n",
    "layer_4_neurons = 10\n",
    "\n",
    "biases = True\n",
    "class Simple_nn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple_nn, self).__init__()\n",
    "\n",
    "        self.fc1 = MetaLinear(28*28*1,layer_1_neurons, bias=biases)\n",
    "        self.fc2 = MetaLinear(layer_1_neurons,layer_2_neurons, bias=biases)\n",
    "        self.fc3 = MetaLinear(layer_2_neurons,layer_3_neurons, bias=biases)\n",
    "        self.fc4 = MetaLinear(layer_3_neurons,layer_4_neurons, bias=biases)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28*1)\n",
    "        x = softplus(self.fc1(x))\n",
    "        x = softplus(self.fc2(x))\n",
    "        x = softplus(self.fc3(x))\n",
    "        x = softplus(self.fc4(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 600\n",
    "\n",
    "mnist_train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=mnist_train_set,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True)\n",
    "mnist_test_loader = torch.utils.data.DataLoader(\n",
    "                dataset=mnist_test_set,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta_LR_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Meta_LR_Model, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(1,128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(128,128)\n",
    "        self.fc4 = nn.Linear(128,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = softplus(self.fc1(x))\n",
    "        x = softplus(self.fc2(x))\n",
    "        x = softplus(self.fc3(x))\n",
    "        x = softplus(self.fc4(x)) * 1e-3\n",
    "        return x \n",
    "\n",
    "meta_lr_model = Meta_LR_Model().cuda()\n",
    "meta_model_opt = optim.Adam(meta_lr_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_num_train = 30 # Number of steps we will train the meta learner \n",
    "mnist_num_train = 3 # We will train on MNIST for this many steps in the inner loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomeOptimizer():\n",
    "  def __init__(self, model):\n",
    "    self.named_buffers = model.named_buffers()\n",
    "    self.model = model\n",
    "\n",
    "  def zero_grad(self):\n",
    "      for name, param in self.named_buffers:\n",
    "          if param.grad:\n",
    "              param.grad.zero_()\n",
    "  \n",
    "  def step(self, meta_output_lr): \n",
    "    for name, param in self.model.named_buffers():\n",
    "      clipping_value = 1e-2\n",
    "      clipped_gradient = torch.clip(param.grad.detach().clone(), min = -clipping_value, max = clipping_value)\n",
    "      # clipped_gradient = param.grad.detach().clone()\n",
    "      \n",
    "      new_param = (param.clone() - meta_output_lr.to(device) * clipped_gradient)\n",
    "      new_param.retain_grad()\n",
    "      rsetattr(self.model, name, new_param)      \n",
    "\n",
    "''' def step(self, layer): \n",
    "    for name, param in self.model.named_buffers():\n",
    "      if layer in name: # layer = 'spec'/'fc'. Basically, only update the parameters for either the specs or the fcs (2 optimizers)\n",
    "        layer_name = name.split('.')[0]\n",
    "        new_param = (param.clone() - learning_rates_dictionary[str(layer_name)].to(device) * param.grad.detach().clone())\n",
    "        new_param.retain_grad()\n",
    "        rsetattr(self.model, name, new_param)\n",
    "\n",
    "The original param is a leaf node, so if we call .backward it will have a gradient.\n",
    "so we create new_param in order to replace that value and still propagate gradients through the new param (otherwise we'll get an error that we changed a leaf node that required grad). \n",
    "We use param.clone() to propagate the gradients from the previous new_param or param if it's the first iteration \n",
    "(but the main reason I used \".clone()\" is because I'm trying to avoid performing in-place operations on leaf nodes). \n",
    "We're doing new_param.retain_grad() because otherwise pytorch will remove the intermediate parameter gradients when we call .backward(). \n",
    "The gradient of the meta learning model will be passed mainly through learning_rates_dictionary. \n",
    "Though, it will can also pass through the gradient of param.grad ( but the paper was saying \n",
    "that this signal is weak so not really needed). \n",
    "The buffers are just \"tensor holders\". \n",
    "That is, we use them because otherwise pytorch will give us an error that we are trying to change inplace an nn.Module. \n",
    "So we create a new set of parameters/tensors using a custom layer that are not nn.Module'''      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_losses = []\n",
    "meta_losses = []\n",
    "predicted_lrs = []\n",
    "\n",
    "# META training loop \n",
    "for outer_loop_epoch in range(meta_num_train):\n",
    "\n",
    "    # reset losses\n",
    "    meta_loss = 0 # meta loss for the meta model\n",
    "\n",
    "    # reset networks\n",
    "    simple_nn = Simple_nn().to(device)\n",
    "\n",
    "    # maintain grad on the parameters of the simple nn\n",
    "    for name, param in simple_nn.named_buffers():\n",
    "        param.retain_grad() \n",
    "\n",
    "    # reset optimizer\n",
    "    opt = CustomeOptimizer(simple_nn) \n",
    "\n",
    "    # MNIST training loop (simple_nn)\n",
    "    for inner_loop_epoch in range(mnist_num_train):        \n",
    "\n",
    "                    # get a batch\n",
    "        for batch_idx, (x, target) in enumerate(mnist_train_loader):\n",
    "            x, target = x.to(device), target.to(device)\n",
    "\n",
    "                        # send inputs to the simple nn\n",
    "            out = simple_nn(x)\n",
    "            fc_loss = criterion(out, target)\n",
    "\n",
    "            # add losses to meta loss for stronger signal\n",
    "            meta_loss += fc_loss \n",
    "        \n",
    "            # get an input to the meta model. We will use the actual loss as the input\n",
    "            meta_model_input = torch.tensor([fc_loss.item()]).to(device) # note that we want to remove the gradient from this as this is just an input\n",
    "\n",
    "                        # get new lrs based on the activation percentages\n",
    "            meta_output_lr = meta_lr_model(meta_model_input)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            fc_loss.backward(retain_graph=True) \n",
    "            opt.step(meta_output_lr)\n",
    "\n",
    "                        # print loss and batch\n",
    "            if (batch_idx) % 200 == 0 or (batch_idx) == len(mnist_train_loader):\n",
    "                print ('==>>> outer loop epoch: {} , inner loop epoch: {} batch index: {}, train loss: {:.6f}'.format(outer_loop_epoch+1,inner_loop_epoch+1, batch_idx, fc_loss.item()))\n",
    "\n",
    "        \n",
    "    ######################################################################################################################################################################################################\n",
    "    # Compute meta loss\n",
    "    ######################################################################################################################################################################################################\\\n",
    "\n",
    "    forgetfulness_loss = meta_loss\n",
    "\n",
    "    meta_model_opt.zero_grad()\n",
    "    forgetfulness_loss.backward() \n",
    "    torch.nn.utils.clip_grad_norm_(meta_lr_model.parameters(), 1e-1)\n",
    "    meta_model_opt.step()\n",
    "\n",
    "    ######################################################################################################################################################################################################\n",
    "    # test\n",
    "    ######################################################################################################################################################################################################\\\n",
    "    with torch.no_grad():\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "            \n",
    "        for batch_idx, (x, target) in enumerate(mnist_test_loader):\n",
    "            x, target = x.to(device), target.to(device)\n",
    "            outputs = simple_nn(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            batch_test_loss = criterion(outputs, target)\n",
    "            test_loss.append(batch_test_loss.item())\n",
    "            test_accuracy.append((predicted == target).sum().item() / predicted.size(0))\n",
    "\n",
    "    ######################################################################################################################################################################################################\n",
    "    # appending stuff (last batch)\n",
    "    ######################################################################################################################################################################################################\\\n",
    "    print('test loss: {}, test accuracy: {}'.format(np.mean(test_loss), np.mean(test_accuracy)))\n",
    "    print('meta_loss', meta_loss.item())\n",
    "    print(f'current predicted learning rate is: {meta_output_lr.item()}')\n",
    "\n",
    "    test_losses.append(np.mean(test_loss))\n",
    "    meta_losses.append(meta_loss.item())\n",
    "    predicted_lrs.append(meta_output_lr.item())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meta losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(meta_losses, c='teal', label='meta_losses')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_losses, c='teal', label='test_losses')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meta model learning rates predictions per loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_function = []\n",
    "\n",
    "for i in torch.range(0, 1, 1e-3): # this is the range for the losses\n",
    "    meta_function.append(meta_lr_model(torch.tensor([i]).cuda()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.xticks([])\n",
    "plt.xlabel('loss steps (0 to 1 in 1e-3 step size)')\n",
    "plt.ylabel('learning rate values')\n",
    "plt.plot(meta_function)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save meta model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and optimizer\n",
    "torch.save({'model': meta_lr_model.state_dict(), 'optimizer_state_dict': meta_model_opt.state_dict()}, 'models_weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the fixed learning rates and the one we just learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train simple neural network using the fixed meta model (not training the meta model)\n",
    "\n",
    "# set seed\n",
    "seed = 888\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# epochs\n",
    "mnist_num_train = 10\n",
    "\n",
    "# lists to hold losses\n",
    "test_losses_meta_trained = []\n",
    "meta_losses = []\n",
    "\n",
    "# set network\n",
    "simple_nn = Simple_nn().to(device)\n",
    "\n",
    "# set optimizer\n",
    "opt = CustomeOptimizer(simple_nn) \n",
    "\n",
    "# MNIST training loop (simple_nn)\n",
    "for inner_loop_epoch in range(mnist_num_train):        \n",
    "\n",
    "    # maintain grad on the parameters of the simple nn\n",
    "    for name, param in simple_nn.named_buffers():\n",
    "        param.retain_grad() \n",
    "        \n",
    "                # get a batch\n",
    "    for batch_idx, (x, target) in enumerate(mnist_train_loader):\n",
    "        x, target = x.to(device), target.to(device)\n",
    "\n",
    "                    # send inputs to the simple nn\n",
    "        out = simple_nn(x)\n",
    "        fc_loss = criterion(out, target)\n",
    "    \n",
    "        # get an input to the meta model. We will use the actual loss as the input\n",
    "        meta_model_input = torch.tensor([fc_loss.item()]).to(device) # note that we want to remove the gradient from this as this is just an input\n",
    "\n",
    "                    # get new lrs based on the activation percentages\n",
    "        meta_output_lr = meta_lr_model(meta_model_input)\n",
    "        \n",
    "        # overwrite existing learning rate\n",
    "        simple_neural_network_lr = meta_output_lr.item() # remove the gradient\n",
    "\n",
    "        opt.zero_grad()\n",
    "        fc_loss.backward(retain_graph=False) \n",
    "        opt.step(torch.tensor(simple_neural_network_lr))\n",
    "\n",
    "                    # print loss and batch\n",
    "        if (batch_idx) % 200 == 0 or (batch_idx) == len(mnist_train_loader):\n",
    "            print ('==>>> inner loop epoch: {} batch index: {}, train loss: {:.6f}'.format(inner_loop_epoch+1, batch_idx, fc_loss.item()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_loss = []\n",
    "        test_accuracy = []\n",
    "            \n",
    "        for batch_idx, (x, target) in enumerate(mnist_test_loader):\n",
    "            x, target = x.to(device), target.to(device)\n",
    "            outputs = simple_nn(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            batch_test_loss = criterion(outputs, target)\n",
    "            test_loss.append(batch_test_loss.item())\n",
    "            test_accuracy.append((predicted == target).sum().item() / predicted.size(0))\n",
    "\n",
    "        print('test loss: {}, test accuracy: {}'.format(np.mean(test_loss), np.mean(test_accuracy)))\n",
    "\n",
    "        test_losses_meta_trained.append(np.mean(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load previous accuracies\n",
    "#### You need to run the simple_neural_network.py 3 times for this!! Once with each unique learning rate.\n",
    "#### That is, run it once with lr 1e-3, once with 1e-2, and once with 1e-1. Change the lr on line 60 in the simple_neural_network.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_neural_network_lr = 1e-3\n",
    "lr_1e_neg_3_accuracy = torch.load(f'test_accuracies_lr_{simple_neural_network_lr}.pt')\n",
    "\n",
    "simple_neural_network_lr = 1e-2\n",
    "lr_1e_neg_2_accuracy = torch.load(f'test_accuracies_lr_{simple_neural_network_lr}.pt')\n",
    "\n",
    "simple_neural_network_lr = 1e-1\n",
    "lr_1e_neg_1_accuracy = torch.load(f'test_accuracies_lr_{simple_neural_network_lr}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.plot(lr_1e_neg_3_accuracy, c='purple', label='1e-3')\n",
    "plt.plot(lr_1e_neg_2_accuracy, c='red', label='1e-2')\n",
    "plt.plot(lr_1e_neg_1_accuracy, c='black', label='1e-1')\n",
    "plt.plot(test_losses_meta_trained, c='teal', label='meta_lr')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c82bff9dcedb9a220d7b6428f77dbad30fd595e6c646f7367c30d64c38714cf"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('meta')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
